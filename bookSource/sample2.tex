\chapter{Sample Second Midterm}

Here are some worked problems typical for what you might expect on a second midterm examination.
\label{sample2}

\begin{enumerate}

\item
{\it Determinants:} The determinant ${\rm det}\, M$ of a $2\times 2$ matrix $M=\begin{pmatrix}a&b\\c&d\end{pmatrix}$ is defined~by
$$
{\rm det}\,  M =ad -bc\, .
$$ 
\begin{enumerate}
\item For which values of ${\rm det}\,  M$ does $M$ have an inverse?
\item Write down all $2\times 2$ bit matrices with determinant~1. (Remember bits are either 0 or 1 and $1+1=0$.)
\item Write down all $2\times 2$ bit matrices with determinant~0.
\item Use one of the above examples to show why the following statement is FALSE.
\begin{quote}
{\it Square matrices with the same determinant are always row equivalent.}
\end{quote}
\end{enumerate}



\item
Let 
$$
A=\left(\begin{array}{ccc}1&1&1\\[2mm]2&2&3\\[2mm]4&5&6\end{array}\right)\, .
$$
Compute $\det A$.
Find all solutions to (i) $A X =  0$ and (ii) $A X=\left(
\begin{array}{c}1\\2\\3\end{array}\right)$ for the vector $X\in \mathbb R^3$. Find, but do not solve,
the characteristic polynomial of $A$.

\item
Let $M$ be any $2\times 2$ matrix. Show
$$
\det M = -\frac 12 {\rm tr} M^2 + \frac 12 ({\rm tr} M)^2\, .
$$

\item
{\it The permanent:} Let $M=(M^i_j)$ be an $n\times n$ matrix. An operation producing a single number from $M$ similar
to the determinant is the ``permanent''
$$
{\rm perm} \, M =\sum_\sigma M^1_{\sigma(1)} M^2_{\sigma(2)}\cdots M^n_{\sigma(n)}\, .
$$
For example
$$
{\rm perm} \begin{pmatrix}a & b \\ c & d\end{pmatrix}=ad+bc\, .
$$
{Calculate} 
$$
{\rm perm} \begin{pmatrix}1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9\end{pmatrix}\, .
$$
\noindent
What do you think would happen to the permanent of an $n\times n$ matrix~$M$ if (include a {\it brief} explanation with each answer):
\begin{enumerate}
\item You multiplied $M$ by a number $\lambda$.
\item You multiplied a row of $M$ by a number $\lambda$.
\item You took the transpose of $M$.
\item  You swapped two rows of $M$.
\end{enumerate}


\item
Let $X$ be an $n\times 1$ matrix subject to
$$
{X}^{T} X=(1)\, ,
$$
and define
$$
H=I - 2 X \,\!X^T\, ,
$$
(where $I$ is the $n\times n$ identity matrix).
Show 
$$
H=H^{T}=H^{-1}.
$$

\item Suppose $\lambda$ is an eigenvalue of the matrix $M$ with associated eigenvector~$v$.
Is~$v$ an eigenvector of $M^k$ (where $k$ is any positive integer)? If so, what would the associated
eigenvalue be?

Now suppose that the matrix $N$ is {\it nilpotent}, {\it i.e.}
$$
N^k=0
$$
for some integer $k\geq 2$. Show that 0 is the only eigenvalue of $N$.

\item
Let $M=\begin{pmatrix}3&-5\\[2mm]1&-3\end{pmatrix}$. Compute $M^{12}$. (Hint: $2^{12}=4096$.)

\item {\it The Cayley Hamilton Theorem}:
Calculate the characteristic polynomial $P_M(\lambda)$ of the matrix $M=\begin{pmatrix}a & b\\c & d\end{pmatrix}$.
Now compute the matrix polynomial $P_M(M)$. What do you observe? Now suppose the $n\times n$ matrix $A$
is ``similar'' to a diagonal matrix $D$, in other words $$A=P^{-1}DP$$ for some invertible matrix $P$ and $D$ is a matrix with values $\lambda_1$, $\lambda_2,\ldots \lambda_n$ along its diagonal. Show that the two matrix polynomials $P_A(A)$ and $P_A(D)$ are similar ({\it i.e.} $P_A(A)=P^{-1} P_A(D) P$).
Finally, compute $P_A(D)$, what can you say about $P_A(A)$?

\item
{\it Define} what it means for a set $U$ to be a subspace of a vector space $V$.
Now let $U$ and $W$ be non-trivial subspaces of $V$. Are the following also subspaces? (Remember that $\cup$ means ``union'' and $\cap$ means ``intersection''.)
\begin{enumerate}
\item $U \cup  W$
\item $U \cap W$ 
\end{enumerate}
In each case {\it draw} examples in $\mathbb R^3$ that justify your answers. If you answered ``yes'' to either part also give a general 
explanation why this is the case.

\item
{\it Define} what it means for a set of vectors $\{v_1,v_2,\ldots,v_n\}$ to (i) be linearly independent, (ii)
  span a vector space~$V$ and (iii)
 be a basis for a vector space~$V$.

Consider the following vectors in $\mathbb R^3$
$$ u =\begin{pmatrix} -1\\ -4\\ 3 \end{pmatrix}\, ,\qquad
     v =\begin{pmatrix} 4\\ 5\\ 0 \end{pmatrix}\, ,\qquad
    w =\begin{pmatrix} 10\\ 7\\ h+3 \end{pmatrix}\, .
$$
For which values of $h$ is $\{u,v,w\}$ a basis for $\mathbb R^3$?



 \end{enumerate}

\subsection*{Solutions}

\begin{enumerate}
\item 
\begin{enumerate}
\item Whenever ${\rm det} M=ad-bc\neq 0$.
\item Unit determinant bit matrices:
$$
\begin{pmatrix}
1&0\\0&1
\end{pmatrix},
\begin{pmatrix}
1&1\\0&1
\end{pmatrix}\, ,
\begin{pmatrix}
1&0\\1&1
\end{pmatrix},
\begin{pmatrix}
0&1\\1&0
\end{pmatrix}\, ,
\begin{pmatrix}
1&1\\1&0
\end{pmatrix},
\begin{pmatrix}
0&1\\1&1
\end{pmatrix}\,.
$$
\item Bit matrices with vanishing determinant:
$$
\begin{pmatrix}
0&0\\0&0
\end{pmatrix},
\begin{pmatrix}
1&0\\0&0
\end{pmatrix}\, ,
\begin{pmatrix}
0&1\\0&0
\end{pmatrix}\, ,
\begin{pmatrix}
0&0\\1&0
\end{pmatrix},
\begin{pmatrix}
0&0\\0&1
\end{pmatrix}\, ,$$ $$
\begin{pmatrix}
1&1\\0&0
\end{pmatrix},
\begin{pmatrix}
0&0\\1&1
\end{pmatrix}\,,
\begin{pmatrix}
1&0\\1&0
\end{pmatrix},
\begin{pmatrix}
0&1\\0&1
\end{pmatrix}\, ,
\begin{pmatrix}
1&1\\1&1
\end{pmatrix}\,.
$$
{\it As a check, count that the total number of $2\times 2$ bit matrices is $2^{(\rm number\  of\  entries)}=2^4=16$.}
\item To disprove this statement, we just need to find a single counterexample. 
All the unit determinant examples above  are actually row equivalent to the identity matrix, so focus on 
the bit matrices with vanishing determinant. Then notice (for example), that
$$
\begin{pmatrix}
1&1\\0&0
\end{pmatrix}{\sim}\!\!\!\!/
\begin{pmatrix}
0&0\\0&0
\end{pmatrix}\, .
$$ 
So we have found a pair of matrices that are not row equivalent but do have the same determinant. 
It follows that the statement is false.
\end{enumerate}




\item
$$
{\rm det }A= 1.(2.6-3.5)-1.(2.6-3.4)+1.(2.5-2.4)=-1\, .
$$
(i) Since ${\rm det}A\neq 0$, the homogeneous system $AX=0$ only has the solution $X=0$.
(ii) It is efficient to compute the adjoint
$$
{\rm adj}\ A= \begin{pmatrix}-3&0& 2\\ -1&2& -1 \\1&-1 & 0 \end{pmatrix}^{\!T}
= \begin{pmatrix}-3&-1& 1\\ 0&2& -1 \\2&-1 & 0 \end{pmatrix}
$$
Hence
$$A^{-1}=\begin{pmatrix}3&1& -1\\ 0&-2& 1 \\-2&1 & 0 \end{pmatrix}\, .$$

Thus
$$
X=\begin{pmatrix}3&1& -1\\ 0&-2& 1 \\-2&1 & 0 \end{pmatrix}
\begin{pmatrix}1\\2\\3
\end{pmatrix}=
\begin{pmatrix}2\\-1\\0
\end{pmatrix}\, .
$$
Finally, 
$$
P_A(\lambda)=-\det \begin{pmatrix}1-\lambda&1&1\\2&2-\lambda&3\\4&5&6-\lambda\end{pmatrix}$$
$$
=-\Big[(1-\lambda)[(2-\lambda)(6-\lambda)-15]-[2.(6-\lambda)-12]+[10-4.(2-\lambda)]\Big]
$$
$$
=\lambda^3-9\lambda^2-\lambda+1\, .
$$
\item
Call $M=\begin{pmatrix}a&b\\c&d\end{pmatrix}$. Then ${\rm det} M= ad-bc$, yet
$$
-\frac 12 \tr M^2 + \frac12 (\tr M)^2 = -\frac 12 \tr \begin{pmatrix}a^2 + bc & * \\ * & bc + d^2\end{pmatrix} -\frac12 (a+d)^2$$ $$ 
=-\frac 12 (a^2 + 2bc + d^2) + \frac 12 (a^2 + 2ad + d^2) = ad - bc\, ,
$$
which is what we were asked to show.

\item 

$$
{\rm perm} \begin{pmatrix}1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9\end{pmatrix}
=1\cdot(5\cdot9+6\cdot8)+2\cdot(4\cdot9+6\cdot7)+3\cdot(4\cdot8+5\cdot7)=450\, .
$$

\begin{enumerate}
\item Multiplying $M$ by $\lambda$ replaces every matrix element $M^i_{\sigma(j)}$ in the formula for the permanent
by $\lambda M^i_{\sigma(j)}$, and therefore produces an overall factor $\lambda^n$.
\item Multiplying the $i^{\rm th}$ row by $\lambda$ replaces $M^i_{\sigma(j)}$ in the formula for the permanent
by $\lambda M^i_{\sigma(j)}$. Therefore the permanent is multiplied by an overall factor $\lambda$.
\item The permanent of a matrix transposed equals the permanent of the original matrix, because
in the formula for the permanent this amounts to summing over permutations of rows rather than columns. But we could
then sort the product $M^{\sigma(1)}_1 M^{\sigma(2)}_2\ldots M^{\sigma(n)}_n$ back into its original order using the inverse permutation
 $\sigma^{-1}$. But summing over permutations is equivalent to summing over inverse permutations, and therefore the permanent is unchanged.
\item Swapping two rows also leaves the permanent unchanged. The argument is almost the same as in the previous part, except 
that we need only reshuffle two matrix elements $M^j_{\sigma(i)}$ and $M^i_{\sigma(j)}$ (in the case where rows $i$ and $j$ were swapped).
Then we use the fact that summing over all permutations $\sigma$ or over all permutations $\widetilde \sigma$ obtained
by swapping a pair in $\sigma$ are equivalent operations.
\end{enumerate}

\item Firstly, lets call $(1)=1$ (the $1\times 1$ identity matrix). Then we calculate
$$
H^T=(I-2 X X^T)^T = I^T -2 (X X^T)^T = I -2 (X^T)^T X^T = I - 2 X X^T = H\, ,
$$
which demonstrates the first equality. Now we compute
$$
H^2 = (I-2 X X^T) (I - 2 X X^T) = I - 4 X X^T + 4 X X^T X X^T $$ $$= I - 4 X X^T + 4 X (X^T X) X^T = I - 4 X X^T + 4 X.  1  .X^T = I\, .
$$
So, since $HH=I$, we have $H^{-1}=H$.

\item We know $Mv=\lambda v$. Hence 
$$
M^2 v = M M v = M \lambda v = \lambda M v = \lambda^2 v\, , 
$$
and similarly
$$
M^k v = \lambda M^{k-1} v = \ldots = \lambda^k v \, .
$$
So $v$ is an eigenvector of $M^k$ with eigenvalue $\lambda^k$.

Now let us assume $v$ is an eigenvector of the nilpotent matrix $N$ with eigenvalue $\lambda$. Then from above
$$
N^k v = \lambda^k v
$$
but by nilpotence, we also have
$$
N^k v = 0.
$$
Hence $\lambda^k v = 0$ and $v$ (being an eigenvector) cannot vanish. Thus $\lambda^k=0$ and in turn $\lambda=0$.

\item Let us think about the eigenvalue problem $Mv=\lambda v$. This has solutions when
$$
0={\rm det} \begin{pmatrix}3-\lambda & -5 \\ 1 & -3-\lambda\end{pmatrix}=\lambda^2-4\Rightarrow \lambda = \pm 2\, .
$$
The associated eigenvalues solve the homogeneous systems (in augmented matrix form)
$$
\left(\begin{array}{cc|c}1 & -5 & 0\\ 1 & -5 & 0\end{array}\right)\sim 
\left(\begin{array}{cc|c} 1 & -5 & 0\\ 0 & 0 & 0\end{array}\right)
\mbox{ and }
\left(\begin{array}{cc|c} 5 & -5 & 0\\ 1 & -1 & 0\end{array}\right)\sim 
\left(\begin{array}{cc|c} 1 & -1 & 0\\ 0 & 0 & 0\end{array}\right)\, ,$$
respectively, so are $v_2=\begin{pmatrix} 5 \\ 1 \end{pmatrix}$ and $v_{-2} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$.
Hence $M^{12} v_2 = 2^{12} v_2$ and $M^{12}v_{-2} = (-2)^{12} v_{-2}$. Now, $\begin{pmatrix} x \\ y \end{pmatrix}=\frac{x-y}{4}\begin{pmatrix} 5 \\ 1 \end{pmatrix} -\frac{x-5y}4  \begin{pmatrix} 1 \\ 1 \end{pmatrix}$
(this was obtained by solving the linear system $a v_2 + b v_{-2} = $ for $a$ and $b$). 
Thus
$$
M \begin{pmatrix} x \\ y \end{pmatrix} = \frac{x-y}{4} M v_2  -\frac{x-5y}4 M v_{-2}$$ $$ = 2^{12} \Big(\frac{x-y}{4}  v_2 -\frac{x-5y}4  v_{-2}\Big) 
= 2^{12} \begin{pmatrix} x \\ y \end{pmatrix}\, .
$$
Thus $$M^{12}=\begin{pmatrix} 4096 & 0 \\ 0 & 4096\end{pmatrix}\, .$$
{\it If you understand the above explanation, then you have a good understanding of diagonalization.  A quicker route  
is simply to observe that~$M^2 = \begin{pmatrix}4 & 0 \\ 0 & 4\end{pmatrix} $.}

\item 
$$
P_M(\lambda) = (-1)^2 {\rm det}\begin{pmatrix} a-\lambda & \mc b  \\ \mc c &d-\lambda\end{pmatrix}
=(\lambda-a)(\lambda-d) - bc\, . 
$$
Thus
$$
P_M(M)=(M-a I )(M- d I) - bc I $$ $$= 
\left(\begin{pmatrix}a&b\\c&d\end{pmatrix}-\begin{pmatrix}a&0\\0&a\end{pmatrix}\right)
\left(\begin{pmatrix}a&b\\c&d\end{pmatrix}-\begin{pmatrix}d&0\\0&d\end{pmatrix}\right)-\begin{pmatrix}bc&0\\0&bc\end{pmatrix}
$$
$$
=\begin{pmatrix}0&\mc b\\c&d-a\end{pmatrix}\begin{pmatrix}a-d&b\\\mc c&0\end{pmatrix}-\begin{pmatrix}bc&0\\0&bc\end{pmatrix}=0\, .
$$
Observe that any $2\times 2$ matrix is a zero of its own characteristic polynomial ({\it in fact this holds for square matrices of any size}).

Now if $A=P^{-1}DP$ then $A^2=P^{-1}DPP^{-1}DP=P^{-1}D^2P$. Similarly $A^k=P^{-1} D^k P$. So for {\it any} matrix polynomial we have
\begin{eqnarray}
&& A^n + c_1 A^{n-1} + \cdots c_{n-1} A + c_n I \nonumber \\ &=& P^{-1}D^nP + c_1 P^{-1}D^{n-1}P + \cdots c_{n-1} P^{-1}DP + c_n P^{-1}P \nonumber \\ &=&
P^{-1}( D^n + c_1 D^{n-1} + \cdots c_{n-1} D + c_n I)P\, .\nonumber
\end{eqnarray}
Thus we may conclude $P_A(A)=P^{-1} P_A(D) P$. 

Now suppose 
$D=\begin{pmatrix}\lambda_1 & \mc0 &\cdots & \mc0 \\ \mc0 &\lambda_2 & &\mc 0\\ \mc\vdots& & \ddots &\mc\vdots \\ \mc0 &&\cdots    &\lambda_n \end{pmatrix}$. Then 
$$P_A(\lambda) = {\rm det} (\lambda I - A) = {\rm det} (\lambda P^{-1} I P - P^{-1} D P) = {\rm det} P . {\rm det} (\lambda I - D). {\rm det} P$$
$$= {\rm det} (\lambda I - D)={\rm det}
 \begin{pmatrix}\lambda-\lambda_1 & \mc0 &\cdots & \mc0 \\[1mm] \mc0 &\lambda-\lambda_2 & &\mc 0 \\ \mc\vdots& & \ddots &\mc\vdots \\ \mc0 &\mc 0&\cdots    &\lambda-\lambda_n \end{pmatrix}$$ $$=(\lambda-\lambda_1)(\lambda-\lambda_2)\ldots (\lambda-\lambda_n)\, .
$$
Thus we see that $\lambda_1$, $\lambda_2, \ldots , \lambda_n$ are the eigenvalues of $M$. Finally we compute 
$$
P_A(D) = (D-\lambda_1)(D-\lambda_2)\ldots (D-\lambda_n)
$$
$$
=\begin{pmatrix}\mc 0 & \mc 0 &\cdots & \mc 0 \\ \mc 0 &\lambda_2 & &\mc 0 \\ \mc\vdots& & \ddots &\mc\vdots \\ \mc 0 &\mc 0&\cdots    &\lambda_n \end{pmatrix}
\begin{pmatrix}\lambda_1 & \mc 0 &\cdots & \mc 0 \\ \mc 0 &\mc 0 & &\mc 0 \\ \mc\vdots& & \ddots &\mc\vdots \\ \mc 0 &\mc 0&\cdots    &\lambda_n \end{pmatrix}
\ldots
\begin{pmatrix}\lambda_1 & \mc 0 &\cdots & \mc 0 \\ \mc 0 &\lambda_2 & &\mc 0 \\ \mc\vdots& & \ddots & \mc\vdots\\ \mc 0 &\mc 0&\cdots    &\mc 0\end{pmatrix}
=0\, .
$$
We conclude the $P_M(M)=0$.


\item  A subset of a vector space is called a subspace if it itself is a vector space, using the rules for vector addition and scalar
multiplication inherited from the original vector space.

\begin{enumerate}
\item So long as  $U\neq U\cup W\neq W$ the answer is {\it no}.  Take, for example, $U$ to be the $x$-axis in ${\mathbb R}^2$
and $W$ to be the $y$-axis. Then $\begin{pmatrix}1,0\end{pmatrix}\in U$ and $\begin{pmatrix}0,1\end{pmatrix}\in W$, but 
$\begin{pmatrix}1,0\end{pmatrix}+\begin{pmatrix}0,1\end{pmatrix}=\begin{pmatrix}1,1\end{pmatrix}\notin U\cup W$.
So $U\cup W$ is not additively closed and is not a vector space (and thus not a subspace). It is easy to draw the example described.
\item Here the answer is always {\it yes}. The proof is not difficult. Take a vector $u$ and $w$ such that $u\in U\cap W\ni w$. This means
that {\it both} $u$ and $w$ are in {\it both} $U$ and $W$. But, since $U$ is a vector space, $\alpha u + \beta w$ is also in $U$.
Similarly, $\alpha u + \beta w \in W$. Hence $\alpha u + \beta w\in U\cap W$. So closure holds in $U\cap W$ and this set is a subspace
by the \hyperref[subspacetheorem]{subspace theorem}. Here, a good picture to draw is two planes through the origin in ${\mathbb R}^3$
intersecting at a line (also through the origin).
\end{enumerate}

\item (i) We say that the vectors $\{v_1,v_2,\ldots v_n\}$ are linearly independent if there exist {\it no} constants $c^1$, $c^2,\ldots c^n$
(not all vanishing) such that $c^1 v_1 + c^2 v_2 +\cdots + c^n v_n=0$. Alternatively, we can require that there is no non-trivial solution for
scalars $c^1$, $c^2,\ldots, c^n $ to  the linear system  $c^1 v_1 + c^2 v_2 +\cdots + c^n v_n=0$.
(ii) We say that these vectors span a vector space $V$ if the set span$\{v_1,v_2,\ldots v_n\}=\{c^1 v_1 + c^2 v_2 +\cdots + c^n v_n:c^1,c^2,\ldots c^n\in  {\mathbb R}\}=V$. (iii) We call $\{v_1,v_2,\ldots v_n\}$ a basis for $V$ if  $\{v_1,v_2,\ldots v_n\}$ are linearly independent {\it and} span$\{v_1,v_2,\ldots v_n\}=V$.

For $u,v,w$ to be a basis for ${\mathbb R}^3$, we firstly need  (the spanning requirement) that any vector $\begin{pmatrix}x \\ y \\ z\end{pmatrix}$ can be written
as a linear combination of $u$, $v$ and $w$
$$
c^1 \begin{pmatrix}-1 \\ -4 \\ 3\end{pmatrix} + c^2 \begin{pmatrix}4 \\ 5 \\ 0\end{pmatrix} + c^3 \begin{pmatrix}\mc{10} \\ \mc{7} \\ h+3\end{pmatrix} = \begin{pmatrix}x \\ y \\ z\end{pmatrix}\, .
$$
The linear independence requirement implies that when $x=y=z=0$, the only solution to the above system is $c^1=c^2=c^3=0$.
But the above system in matrix language reads
$$
\begin{pmatrix}
-1 &4 &\mc{10} \\ -4 & 5 &\mc{7} \\ 3 & 0 & h+3
\end{pmatrix}
\begin{pmatrix}c^1 \\ c^2 \\ c^3\end{pmatrix}=\begin{pmatrix}x \\ y \\ z\end{pmatrix}\, .
$$
Both requirements mean that the matrix on the left hand side must be invertible, so we examine its determinant
$$
{\rm det} \begin{pmatrix}
-1 &4 &\mc{10} \\ -4 & 5 &\mc{7} \\ 3 & 0 & h+3
\end{pmatrix}
= -4\cdot (-4\cdot(h+3)-7\cdot3)+ 5\cdot(-1\cdot(h+3)-10\cdot3)$$ $$=11(h-3)\, \cdot
$$
Hence we obtain a basis whenever $h\neq 3$.

\end{enumerate}

\newpage
