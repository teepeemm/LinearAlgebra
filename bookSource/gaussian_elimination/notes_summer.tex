\chapter{\gaussElimTitle}

\label{gaussElim}

You have seen that systems of linear equations can be written as matrix equations.
You will see that Gaussian elimination is an efficient \gls{shorthand} for (maximally) simplifying a system of linear equations (or matrix equation).

%You might get the feeling that you are learning to do Gaussian elimination only so that you can tell your computer how too do it in the future. There is more than that going on here. Let us foreshadow chapter 3; as we attempt to streamline the process of elimination we will discover the building blocks of matrices. 



%In \Lecture~\ref{warmup}  we looked performed elimination on a system of equations. 
%It was nice to do this elimination by adding subtracting an multiplying wholes equations at a time. 
%Lets do another example with the system
%\begin{eqnarray}
%	x\ +\ y & = & 27\nn \\
%	2x-\ y & = &\  0\nn \,.
%\end{eqnarray}
%Adding the first equation to the second then dividing by 3 gives
%\begin{eqnarray}
%	x\ +\ 0 & = & 9\nn \\
%	2x-\ y & = &\  0\nn \,.
%\end{eqnarray}
%Subtracting rice the first from the second then dividing by $-1$ gives
%\begin{eqnarray}
%	x \phantom{\ +\ y}& = &\  9\nn \\
%	\phantom{x\ +\ }y & = & 18\, .\nn
%\end{eqnarray}
%
%\noindent
%The maximum number of terms have been eliminated from each equation, and the result is an obvious statement of the solution. 
%\\
%
%We also learned to write such a linear system using a matrix and two vectors. In this case the original linear system can be written
%
%\begin{equation*}
%    \begin{pmatrix}
%      1             &1  \\
%      2             &-1
%    \end{pmatrix}
%  \colvec{x \\ y}
%  =
%  \colvec{27 \\ 0}\, .
%\end{equation*}
%
%
%\noindent
%Likewise, the system of equations that we obtained after elimination can be written
%
%\begin{equation*}
%    \begin{pmatrix}
%      1             &0  \\
%      0             &1
%    \end{pmatrix}
%  \colvec{x \\ y}
%  =
%  \colvec{9 \\ 18} \, .
%\end{equation*}
%\\
%
%%\noindent
%By the way, the matrix $$I=    \begin{pmatrix}
%      1             &0  \\
%      0             &1
%    \end{pmatrix}$$ is called the \emph{Identity Matrix}\index{Identity matrix!$2\times2$}.  She will be very important to us. You should check that if $v$ is any vector, then $$Iv=v\, .$$
%    
%Here is a nice way to summarize your goal when performing the process we are calling elimination elimination; manipulate the system of equations until the resulting system can be written as a matrix equation with the identity matrix.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Augmented Matrix Notation for Linear Systems}

%A useful shorthand for a  system of linear equations is an \hypertarget{augmented_matrix}{\emph{Augmented Matrix}}\index{Augmented matrix~$2\times2$}. 
We will introduce this notation through examples: 
The linear system 
\begin{eqnarray}
	x\ +\ y & = & 27\nn \\
	2x-\ y & = &\  0\nn \,
\end{eqnarray}
is denoted by the augmented matrix

\[
\begin{amatrix}{2}
1 &1 &27 \\ 2 &-1 & 0
\end{amatrix}
\]

\noindent
This is truly minimalist notation. It is even more minimal then the matrix notation  
\begin{equation*}
    \begin{pmatrix}
      1             &1  \\
      2             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{27 \\ 0}\, .
\end{equation*}
All three of the above equations denote the same thing. 


\videoscriptlink{gaussian_elimination_more_background.mp4}{Augmented Matrix Notation}{script_gaussian_elimination_more}

Going up in size,
the system
\begin{eqnarray*}
1x + 3y + 2z + 0w   =9 \\ 
6x + 2y + 0z   -2w  =0  \\
-1x+ 0 y + 1 z + 1w  =3 
\end{eqnarray*}
is denoted by the augmented matrix
\[
\begin{amatrix}{4}
1 & 3 & 2 & 0  & 9 \\ 
6 & 2 & 0  & -2 & 0  \\
-1& 0  & 1  & 1 & 3
\end{amatrix}
\]
%When writing a system of equations one can write out the terms with zero for coefficients or not
%\begin{eqnarray*}
%1x + 3y + 2z  \phantom{+ 0w}   =9 \\ 
%6x + 2y \phantom{ + 0z}   -2w  =0  \\
%-1x  \phantom{+0 y} + 1 z + 1w  =3 
%\end{eqnarray*}
As a matrix equation the system reads
\begin{eqnarray*}
\left(\begin{array}{cccc}
1 & 3 & 2 & 0   \\ 
6 & 2 & 0  & -2   \\
-1& 0  & 1  & 1 
\end{array}\right)
\colvec{ x\\ y\\z\\w}
=\colvec{ 9\\0\\3}
\end{eqnarray*}




Here's the general case:  The number of equations in the linear system is the number of rows $r$ in the augmented matrix, and the number of columns $k$ in the matrix left of the vertical line is the number of unknowns.
\[
\begin{amatrix}{4}
a^1_1 & a^1_2 & \cdots & a^1_k & b^1 \\ 
a^2_1 & a^2_2 & \cdots & a^2_k & b^2 \\ 
\vdots & \vdots & & \vdots & \vdots  \\
a^r_1 & a^r_2 & \cdots & a^r_k & b^r \\ 
\end{amatrix}
\]
Entries left of the divide carry two indices; subscript denotes column number, superscript denotes row number. We emphasize, the superscripts here do not denote exponents.  

%Aside: most people don't like  indexing by superscripts at first. However, kind of index placement will later facilitate Einstein's summation convention, which Einstein himself described as his greatest contribution to the sciences. 

Make sure you can write out the system of equations and the associated matrix equation for any augmented matrix. 
\reading{2}{1}




We now have three ways or writing the same question. 
Lets put them side by side as we solve the system using a nice algebra trick: we will strategically add and subtract equations.

\begin{example}  (how matrix equations and augmented matrices change in elimination)
\begin{eqnarray*}
   \left.
\begin{array}{lcr}
	x\ +\ y & = & 27 \\
	2x-\ y & = &\  0 
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      1             &1  \\
      2             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{27 \\ 0}
  \Leftrightarrow
 \begin{amatrix}{2}
1 &1 &27 \\ 2 &-1 & 0
\end{amatrix}
  \end{eqnarray*}
Let the new first equation be the sum of the first and second equations
\begin{eqnarray*}
   \left.
\begin{array}{lcr}
	3x\ +\ 0 & = & 27 \\
	2x-\ y & = &\  0 
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      3             &0  \\
      2             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{27 \\ 0}
  \Leftrightarrow
 \begin{amatrix}{2}
3 &0 &27 \\ 2 &-1 & 0
\end{amatrix}
  \end{eqnarray*}
Let the new first equation be the old first equation divided by 3
 \begin{eqnarray*}
   \left.
\begin{array}{lcr}
	x\ +\ 0 & = & 9 \\
	2x-\ y & = &\  0 
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      1             &0  \\
      2             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{9 \\ 0}
  \Leftrightarrow
 \begin{amatrix}{2}
1 &0 &9 \\ 2 &-1 & 0
\end{amatrix}
  \end{eqnarray*}
Let the new second equation be the old second equation minus two times the first equation 
\begin{eqnarray*}
   \left.
\begin{array}{lcr}
	x\ +\ 0 & = & 9 \\
	0-\ y & = &\  -18
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      1             &0  \\
      0             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{9 \\ -18}
  \Leftrightarrow
 \begin{amatrix}{2}
1 &0 &9 \\ 0 &-1 & -18
\end{amatrix}
  \end{eqnarray*}
Let the new  second equation be the old second equation divided by -1
\begin{eqnarray*}
   \left.
\begin{array}{lcr}
	x + 0 & = & \ 9 \\
	0 + y & = &  18
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      1             &0  \\
      0             &1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{9 \\ 18}
  \Leftrightarrow
 \begin{amatrix}{2}
1 &0 &9 \\ 0 &1 & 18
\end{amatrix}
  \end{eqnarray*}
\end{example}
Did you see what the strategy was? To eliminate $y$ from the first question and then eliminate $x$ from the second. The result was a plain statement of the solution. But the quantity of writing here was atrocious. 

Here is the big idea: 
Everywhere in the the instructions we can replace the word ``equation" with the word ``row" and interpret the instructions as telling us what to do with the augmented matrix instead of the system of equations.
The result is a process called {\it Gaussian elimination}\index{Gaussian elimination}: 
%
%\begin{example} of Gaussian elimination
%\begin{eqnarray*}
% \begin{amatrix}{2}
%1 &1 &27 \\ 2 &-1 & 0
%\end{amatrix}
%  \end{eqnarray*}
%Let the new first row be the sum of the first and second rows
%\begin{eqnarray*}
% \begin{amatrix}{2}
%3 &0 &27 \\ 2 &-1 & 0
%\end{amatrix}
%  \end{eqnarray*}
%Let the new first row be the old first row divided by 3
% \begin{eqnarray*}
% \begin{amatrix}{2}
%1 &0 &9 \\ 2 &-1 & 0
%\end{amatrix}
%  \end{eqnarray*}
%Let the new second row be the old second row minus two times the first row 
%\begin{eqnarray*}
% \begin{amatrix}{2}
%1 &0 &9 \\ 0 &-1 & -18
%\end{amatrix}
%  \end{eqnarray*}
%Let the new  second row be the old second row divided by -1
%\begin{eqnarray*}
% \begin{amatrix}{2}
%1 &0 &9 \\ 0 &1 & 18
%\end{amatrix}
%  \end{eqnarray*}
%The solution can be read off very quickly, and the notation was very minimal. 
%\end{example}
% At each step, the augmented matrices encode systems of equations which have the same solutions. Lets make this idea more formal, and introduce some notation to convey the idea.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Equivalence and the Act of Solving}


%Two augmented matrices corresponding to linear systems 
%%{\it that actually have solutions} %cmon, this idea has not even been introduced
%are said to be \hypertarget{roweq}(row) 
%\emph{equivalent}\index{Row equivalence} if they have the \emph{same} solutions.
%To denote this 

We introduce the symbol $\sim$ which is called ``tilde" but should be read as  ``is equivalent to''. For example we found above that
\[
\begin{amatrix}{2}
1 &1 &27 \\ 2 &-1 &0
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &0 &9 \\ 2 &-1 & 0
\end{amatrix}
\sim
\begin{amatrix}{2}
1 & 0&9 \\   0& 1 & 18
\end{amatrix}
\]
and certainly the last of these is our favorite!
\videoscriptlink{gaussian_elimination_background.mp4}{Equivalence Example}{script_gaussian_elimination_background}

%\videoscriptlink{gaussian_elimination_3_3_example.mp4}{A $3 \times 3$ example}{scripts_gaussian_elimination_3_3_example}

Setting up a string of equivalences like this is a means of solving a system of linear equations. This is the main idea of chapter 2.

\begin{example} (Using Gaussian elimination to solve a system of linear equations)
\begin{eqnarray*}
   \left.
\begin{array}{lcr}
	x +\ y & = & 5 \\
	x + 2y & = &\  8
     \end{array}
   \right\} 
   \Leftrightarrow
\begin{amatrix}{2}
1 &1 &5 \\ 1 &2 & 8
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &1 &5 \\ 0 &1 & 3
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &0 &2 \\ 0 &1 & 3
\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{lcr}
	x + 0 & = & 2 \\
	 0 + y & = &\  3
     \end{array}
   \right.
\end{eqnarray*}  
Note that we used the top left $1$ to make the bottom left entry zero. For this reason we call that number a pivot; imagine a wiper blade hanging down form the top left slot wiping the lower left slot away. Similarly the bottom right slot was used to wipe away the top right slot, so it is called a pivot. 
\end{example}

This {\it pivot} jargon is helpful for describing the goings on in Gaussian elimination. Be sure you understand the idea before moving on. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Reduced Row Echelon Form}
For a system of two linear  equations, the goal in Gaussian elimination is to have the part of the augmented matrix left of the dividing line become the matrix
 $$I=    \begin{pmatrix}
      1             &0  \\
      0             &1
    \end{pmatrix}$$ 
called the \emph{Identity Matrix}\index{Identity matrix!$2\times2$}, since this would give the simple statement of a solution $x=a,y=b$. The situation is similar for larger systems of equations.
\reading{2}{2} %a 3x3 example with one solution>



\noindent
This can't always be done.

\begin{example} (Redundant equations)
 \begin{eqnarray*}
   \left.
\begin{array}{lcr}
	\ x + \ y & = & 2 \\
	2x + 2y & = &  4
     \end{array}
   \right\} 
   \Leftrightarrow
\begin{amatrix}{2}
1 &1 &2 \\ 2 &2 & 4
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &1 &2 \\ 0 &0 & 0
\end{amatrix}
%\sim
%\begin{amatrix}{2}
%1 &0 &2 \\ 0 &1 & 3
%\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{lcr}
	x + y & = & 2 \\
	 0 + 0 & = &  0
     \end{array}
   \right.
\end{eqnarray*}  
This example demonstrates if one equation is a multiple of the other the identity matrix can not be a reached. This is because the first step in elimination will make the second row a row of zeros. Notice that solutions still exists $x=1,y=1$ is a solution. The last augmented matrix here is in RREF.
\end{example}

\begin{example} (Inconsistent equations)
 \begin{eqnarray*}
   \left.
\begin{array}{lcr}
	\ x + \ y & = & 2 \\
	2x + 2y & = &  5
     \end{array}
   \right\} 
   \Leftrightarrow
\begin{amatrix}{2}
1 &1 &2 \\ 2 &2 & 5
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &1 &2 \\ 0 &0 & 1
\end{amatrix}
%\sim
%\begin{amatrix}{2}
%1 &0 &2 \\ 0 &1 & 3
%\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{lcr}
	x + y & = & 2 \\
	 0 + 0 & = &  1
     \end{array}
   \right.
\end{eqnarray*}  
This system of equation has a solution if there exists two numbers $x$, and $y$ such that $0+0=1$. That is a tricky way of saying there are no solutions. The last form of the augmented matrix here is in RREF.
\end{example}


\begin{example} (Silly order of equations)\\
A robot might make the following mistake
 \begin{eqnarray*}
   \left.
\begin{array}{lcr}
	0x +  \ y & = & -2 \\
	\ x + y & = &  7
     \end{array}
   \right\} 
   \Leftrightarrow 
\begin{amatrix}{2}
0 &1 & -2\\ 1 &1 & 7
\end{amatrix}
\sim \cdots
\end{eqnarray*}  
and then give up because the the upper left slot can not function as a pivot since the 0 that lives there can not be used to eliminate the zero below it. Of course, the right thing to do is to change the order of the equations before starting
 \begin{eqnarray*}
   \left.
\begin{array}{lcr}
	\ x + y & = &  7
	\\
	0x +  \ y & = & -2 
	     \end{array}
   \right\} 
   \Leftrightarrow
\begin{amatrix}{2}
1 &1 & 7\\ 0 &1 & -2
\end{amatrix}
\sim 
\begin{amatrix}{2}
1 &0 & 9\\ 0 &1 & -2
\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{lcr}
	x + 0 & = & 9 \\
	 0 + y & = & -2
     \end{array}
   \right.
\end{eqnarray*}  
The third appearance of an augmented matrix here is RREF of the first, and second. That is to say, you can swap rows on your way to RREF.
\end{example}



For larger systems of matrices, these three kinds of problems are the obstruction to obtaining the identity matrix, and hence to a simple statement of a solution in the form $x=a,y=b,...$. 
What can we do to maximally simplify a system of equations in general?
We can do three things corresponding to exchanging the order of equations, multiplying one equation by a constant or adding equations:
\begin{itemize}
\item (Row Swap) Exchange any two rows.
\item (Scalar Multiplication) Multiply any row by a non-zero constant.
\item (Row Sum) Add a multiple of one row to another row.
\end{itemize}
These are called the \emph{Elementary Row Operations}\index{EROs}, or EROs for short. They are the  subject of the next lecture. 
They can be used to achieve \emph{Reduced Row Echelon Form}\index{Reduced row echelon form} (RREF), the maximally simplified augmented matrix, which typically looks like 
\[
\begin{amatrix}{7}
1       	& * & 0		& * & 0		& \cdots& 0		& b^1 \\ 
0	        & 0 & 1		& * & 0		& \cdots& 0		& b^2 \\
0		& 0& 0		& 0 & 1		& \cdots& 0		& b^3 \\  
\vdots  	& \vdots& \vdots	&   & \vdots	& 	& 0			& \vdots \\  
		& &			&  &			&      & 1			& b^k \\  
0		& 0 & 0		& 0 & 0		& \cdots& 0 		& b^{k+1} \\ 
\vdots  	& \vdots & \vdots	&  \vdots & \vdots	& 	& \vdots		& \vdots \\  
0		&  0 & 0		& 0 & 0		& \cdots& 0		& b^r \\ 
\end{amatrix}
\]

%The first non-zero entry in each row is called the \emph{pivot}\index{Pivot}.    
\noindent
The asterisks denote the possibility of arbitrary numbers. (For e.g. 1 in the redundant equations example.)  
If any of the numbers $b^{k+1},\dots b^r$ are non-zero then the system of equations is inconsistent and has no solutions. 


It is essential to be able to visualize how 
RREF can be obtained using (only) elementary row operations in the most general case:
\begin{itemize}
\item Make the leftmost nonzero entry in the top row 1 by multiplication.  
\item Then use that 1 as a pivot to eliminate everything below it. 
\item Then go to the next row and make the leftmost non zero entry 1. 
\item Then use that 1 as a pivot to eliminate everything below {\it and above it}! 
\item Then go to the next row and make the leftmost nonzero entry 1... etc
\end{itemize}
Note that this is not the only way to get to RREF. You might, for example, begin by eliminating everything below the top left entry before setting it to one. 

\videoscriptlink{gaussian_elimination_Beginner_Elimination.mp4}{Beginner Elimination}{script_gaussian_elimination_more}

\noindent
The following properties fully describe  RREF.

\begin{enumerate}
\item  In every row  the left most non-zero entry is  $1$ (and is called a pivot).

\item The pivot of any given row is always to the right of the pivot of the row above it.

\item The pivot is the only non-zero entry in its column.
\end{enumerate}

\begin{example} (Augmented matrix in RREF)
$$
\begin{amatrix}{3} 
1 & 0 & 7 & 0 \\ 
0 & 1 & 3 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 \\
\end{amatrix}
$$
\end{example}

\begin{example} (Augmented matrix NOT in RREF)
$$
\begin{amatrix}{3} 
1 & 0 & 3 & 0 \\ 
0 & 0 & 2 & 0 \\
0 & 1 & 0 & 1 \\
0 & 0 & 0 & 1 \\
\end{amatrix}
$$
Actually, this NON-example breaks all three of the rules!
\end{example}


The reason we need the asterisks in the general form of RREF:
it is not necessary for every column to have a pivot as demonstrated in the ``augmented matrix in RREF" and ``redundant equations" examples above. 
Actually, in full generality where we have asterisks one can have multiple columns with no pivot as one would have in the following

\begin{example} (Consecutive  columns with no pivot in RREF)
 \begin{eqnarray*}
   \left.
\begin{array}{lcr}
	\ x \ + \ y + \ z +  0w & = & 2 \\
	2x + 2y +2z+2w & = &  4
     \end{array}
   \right\} 
   \Leftrightarrow
\begin{amatrix}{4}
1 &1 &1 & 0 &2 \\ 
2 &2 &2 & 1 & 4
\end{amatrix}
\\
\sim
\begin{amatrix}{4}
1 &1 &1 & 0 &2 \\ 
0 &0 &0 & 1 & 0
\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{lcr}
	x + y +z& = & 2 \\
	 w & = &  0
     \end{array}
   \right.
\end{eqnarray*}  
Note that there was no hope of reaching the identity matrix just because of the shape of the augmented matrix we started with. 
\end{example}


\videoscriptlink{gaussian_elimination_Advanced_Elimination.mp4}{Advanced Elimination}{script_gaussian_elimination_more}



\section{Solution Sets}
%While RREF is not always pretty, it is certainly useful. 
%Our goal  is to solve systems of linear equations. 
RREF represents a maximally simplified version of the original system of equations in the following sense: 
\begin{itemize}
\item As many coefficient of the variables have been set to zero as is possible 
\item As many coefficients of variables have been set to 1 as possible.
\end{itemize}
It is easier to read off solutions from the maximally simplified equations than from the original equations, even when there are infinitely many solutions.

\begin{example}
 \begin{eqnarray*}
\left.
\begin{array}{lcr}
	x  +y    \phantom{+ z}  + 5w & ~  = 1 \\
	\phantom {x+}  \ y  \phantom{+z}   + 2 w & = 6 \\
	\phantom{x+y+} z+         4w & = 8
\end{array}
 \right\}
 \Leftrightarrow
 \begin{amatrix}{4} 
1 & 1 & 0 & 5 & 1 \\ 
0 & 1 & 0 & 2 & 6 \\
0 & 0 & 1 & 4 & 8 
\end{amatrix}
\sim
 \begin{amatrix}{4} 
1 & 0 & 0 & 3 & -5 \\ 
0 & 1 & 0 & 2 & 6 \\
0 & 0 & 1 & 4 & 8 
\end{amatrix}
\\
\Leftrightarrow
\left\{
\begin{array}{lcr}
	x \phantom{+y    + z}  + 3w & ~ \ = -5 \\
	\phantom {x+}   y \, \phantom{+z}  \ + 2 w & = 6 \\
	\phantom{x+y+} z+         4w & = 8
     \end{array}
     \right.
\end{eqnarray*}
In this case, we say that $x,y$, and $z$ are {\it pivot variables}\index{pivot variables} because they appear with pivots as coefficients. Since $w$ never appears with a pivot as a coefficient, 
we say that it is not a pivot variable. %We call it a free variable. 
One way to express the solutions to this system of equations is to put all the pivot variables on one side and all the {\it non-pivot variables}\index{non-pivot variables} on the other side. It is also nice to add the ``freebee equation" $w=w$ to obtain the system
\begin{eqnarray*}
\left.
\begin{array}{lcr}
	x & = -5 -3w \\
	 y  & = 6 -2w\\
	 z & = 8-4w \\
	w & =\phantom{8-4}w          
     \end{array}
     \right\}
     \Leftrightarrow
\colvec{x\\y\\z\\w} = \colvec{-5\\6\\8\\0} + w\colvec{-3\\-2\\-4\\1}
\end{eqnarray*}
where we have written the solution to the corresponding matrix problem. There are infinitely many solutions, one for each value of $z$. We call the collection of all solutions {\it the solution set}\index{solution set}\index{solution set} It is easy to check that the solution corresponding to $w=0$ really does fit into the matrix equation. 
\end{example}


The last example demonstrated the \hypertarget{standard approach}{{\it standard approach}} to solving a system of linear equations in its entirety: write the augmented matrix, obtain RREF, return to the system of equations, and express the non-pivot variables in terms of the pivot variables. 
There are always exactly enough non-pivot variables to index your solutions. 
In any approach, the variables which are not expressed in terms of the other variables are called the {\it free variables}\index{free variables}. The standard approach is to use the non-pivot variables as free variables.

%vid for this?! 
Non-standard approach: solve for $w$ in terms of $z$ and substitute into the other equations. You now have an expression for each component in terms of $z$. But why pick $z$ instead of $y$ or $x$? (or $x+y$?) The standard approach feels natural, and that is why it became the standard.

When you see a RREF augmented matrix with two columns that have no pivot, you know there will be two free variables. 

\begin{example}
 \begin{eqnarray*}
 \begin{amatrix}{4} 
1 & 0 & 7 & 0 & 4 \\ 
0 & 1 & 3 & 4 & 1 \\ 
0 & 0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0 & 0 \\ 
\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{lcr}
	x \phantom{+y}    + 7z  \phantom{+w} & = 4 \\
	\phantom {x+}   y + 3z  {+4w} & = 1 \\
	%\phantom{x+y+z+}          w & = 2
     \end{array}
     \right.
\end{eqnarray*}  
Expressing the pivot variables in terms of the non-pivot variables, and using two freebee equations gives
\begin{eqnarray*}
\left.
\begin{array}{lcr}
	x & = 4 -7z \\
	 y  & = 1 -3z-4w\\
	 z         & = z\\
	w & =w          
     \end{array}
     \right\}
     \Leftrightarrow
\colvec{x\\y\\z\\w} = \colvec{4\\1\\0\\0} + z\colvec{-7\\-3\\1\\0} + w\colvec{0\\-4\\0\\1}
\end{eqnarray*}
There are infinitely many solutions. One solution for each pair of numbers $z,w$. 
\end{example}
You can imagine having three, four, or fifty-six non-pivot columns and the same number of free variables indexing your solutions set. You need to become very adept at reading off solutions of linear systems from the RREF
of their augmented matrix. 

\videoscriptlink{solution_sets_for_systems_example.mp4}{Solution set in set notation}{solution_sets_for_systems_of_linear_equations_example}

\videoscriptlink{elementary_row_operations_worked_examples.mp4}{\hspace{-8mm}Worked examples of Gaussian elimination\hspace{-8mm}}{scripts_elementary_row_operations_worked_examples}


%This example emphasizes different aspects.
%\begin{example}
%$$
%\begin{amatrix}{5} 
%1 & 1 & 0 & 1 & 0 & 1\\ 
%0 & 0 & 1 & 2 & 0 & 2\\ 
%0 & 0 & 0 & 0 & 1 & 3\\ 
%0 & 0 & 0 & 0 & 0 & 0
%\end{amatrix}\, .
%$$
%Here we were not told the names of the variables, so lets just call them $x_1,x_2,x_3,x_4,x_5$.
%(There are always as many of these as there are columns in the matrix before the vertical line; the number of rows,
%on the other hand is the number of linear equations.)
%
%To begin with we immediately notice that there are no pivots in the second and fourth columns so, as per the standard approach, we will be all variables in terms of $x_2$ and $x_4$. 
%Next we see from the second last row that $x_5=3$. The second row says 
%$x_3=2-2x_4=2-2 x_2$.
%The top row then gives $x_1=1-x_2-x_4=1-x_1-x_2$. Again we can write this solution as a vector
%$$
%\colvec{1\\0\\2\\0\\3}+x_1\colvec{-1\\1\\0\\0\\0}+x_2\colvec{-1\\0\\-2\\1\\0}\, .
%$$
%Observe, that since no variables were given at the beginning, we can use any symbols instead of $x_1$ and $x_2$. For example lower case greek letter lambda:
%$$
%\colvec{1\\0\\2\\0\\3}+\lambda_1\colvec{-1\\1\\0\\0\\0}+\lambda_2\colvec{-1\\0\\-2\\1\\0}\, .
%$$
%As a challenge, look carefully at this solution and make sure you can see how every part of it comes from
%the original augmented matrix without every having to reintroduce variables and equations.
%\end{example}
%
%




%\begin{theorem}
%Every augmented matrix is row-equivalent to a \emph{unique} augmented matrix in reduced row echelon form.
%\end{theorem}
%
%\noindent
%In \Lecture~\ref{elemRowOpsPath}, we will see why this is true.

%\section*{Uniqueness of RREF}
%
%\begin{theorem}\label{GJEunique} Gauss-Jordan Elimination produces a unique augmented matrix in RREF.
%\end{theorem}
%
%\begin{proof}
%Suppose Alice and Bob compute the RREF for a linear system but get different results, $A$ and $B$.  Working from the left, discard all columns except for the pivots and the first column in which $A$ and $B$ differ.  By \hyperref[colremove]{Review Problem~\ref{colremove}}, removing columns does not affect row equivalence.  Call the new, smaller, matrices $\hat{A}$ and $\hat{B}$.  The new matrices should look this: $$\hat{A}=\begin{amatrix}{1}
%I_N & a\\
%0 & 0
%\end{amatrix} \mbox{ and } \hat{B}=\begin{amatrix}{1}
%I_N & b\\
%0 & 0
%\end{amatrix}\, ,$$ where $I_N$ is an $N\times N$ identity matrix and $a$ and $b$ are vectors.
%
%Now if $\hat{A}$ and $\hat{B}$ have the same solution, then we must have $a=b$.  But this is a contradiction!  Then $A=B$.
%\end{proof}
%
%\videoscriptlink{elementary_row_operations_proof.mp4}{Explanation of the proof}{scripts_elementary_row_operations_proof}


%\References{
%Hefferon, Chapter One, Section 1
%\\
%Beezer, Chapter SLE, Section RREF
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/Row_echelon_form}{Row Echelon Form}}

%\newpage

\section{Review Problems}

\input{\gaussElimPath/problems}

\newpage

