

\begin{enumerate}

\item Write down examples of augmented matrices corresponding to each of the \hyperlink{FIVE}{five types of solution sets} for systems of equations with three unknowns.

%%%%%%%%%%%%%%%%%%%%%%

\item Invent  a simple linear system that has multiple solutions. Use the \hyperlink{standard approach}{standard approach} for solving linear systems and a non-standard approach to obtain  different descriptions of the solution set.  Is the solution set different with different approaches? 

%%%%%%%%%
%\item \label{linear} Let $f(X)=MX$ where
%$$
%M=    \begin{pmatrix}
%      1  & 0  & 1 & \!\!-1  \\
%       0  & 1 & \!\!-1 & 1  \\
%        0 &0   & 0  & 0    \\
%    \end{pmatrix}  \mbox{ and }
%X= \colvec{x_1\\x_2\\x_3\\x_4}\, ,\quad Y= \colvec{y_1\\y_2\\y_3\\y_4}\, .
%$$
%Suppose that $\alpha$ is any number. Compute the following four quantities: $$\alpha X\, , \ f(X)\, , \  \alpha f(X) \mbox{ 
%and } f(\alpha X)\, .$$ Check your work by verifying that $$\alpha f(X) = f(\alpha X)\, ,\quad \mbox{and}f(X+Y)=f(X)+f(Y)\, \, .$$
%Now explain why  your results for
%$f(\alpha X)$ and $f(X+Y)$ together imply $$f(\alpha X + \beta Y)= \alpha f(X) + \beta f(Y)\, .$$
%(Be sure to state which values of the scalars $\alpha$ and $\beta$ are allowed.)
%
%%%%%%%%%%%%%%%%%%%%%
\item \label{matvect} Let
\label{matrixmult}
\[ M = \begin{pmatrix}
      a^1_1 & a^1_2  & \cdots & a^1_k  \\[1mm]
      a^2_1 & a^2_2  & \cdots & a^2_k  \\[1mm]
      \multicolumn{1}{c}{\vdots} & \multicolumn{1}{c}{\vdots}   &     & \multicolumn{1}{c}{\vdots}  \\[1mm]
      a^r_1 & a^r_2  & \cdots & a^r_k  
    \end{pmatrix}\ \mbox{ and } \ x=\ccolvec{x^1\\[1mm]x^2\\[1mm] \vdots \\[1mm]x^k}
    \, .
\]

Note: $x^2$ does not denote the square of the column vector $x$. Instead $x^1$, $x^2$, $x^3$, {\it etc...},
denote different variables (the components of $x$); the superscript is an index. 
Although confusing at first, this notation was invented by Albert Einstein\index{Einstein, Albert}
who noticed that quantities like $a^2_1 x^1   + a^2_2 x^2   \cdots   + a^2_k x^k=:\sum_{j=1}^k a^2_j x^j$, can be written unambiguously  
as $a^2_j x^j$. This is called \href{http://en.wikipedia.org/wiki/Einstein_notation}{Einstein summation notation}.
The most important thing to remember is that the index $j$ is a dummy variable, so that $a^2_j x^j\equiv a^2_i x^i$; this is called ``relabeling dummy indices''.
When dealing with products of sums, you must remember to introduce a new dummy for each term; {\it i.e.}, $a_i x^i b_iy^i = \sum_i a_i x^i b_i y^i$ does {\it not} equal $a_i x^i b_jy^j = \big(\sum_i a_ix^i\big)\big(\sum_j b_j y^j\big)$.
 


Use Einstein summation notation to propose a rule for $Mx$ so that $Mx=0$ is equivalent to the linear system 
    \begin{equation*}
      \begin{linsys}{2}
            a^1_1 x^1  & + a^1_2 x^2  & \cdots  & + a^1_k x^k  & =0  \\[1mm]
            a^2_1 x^1  & + a^2_2 x^2  & \cdots  & + a^2_k x^k  & =0  \\[1mm]
	    \vdots     & \vdots       &         & \vdots       & \vdots\  \,  \\[1mm]
            a^r_1 x^1  & + a^r_2 x^2  & \cdots  & + a^r_k x^k  & =0  \\
      \end{linsys}
    \end{equation*}
Show that your rule for multiplying a matrix by a vector obeys the linearity property.



% \it closing brace

%\videoscriptlink{solution_sets_for_systems_of_linear_equations_hint.mp4}{Problem~\ref{matvect} hint}{scripts_solution_sets_for_systems_of_linear_equations_hint}
 
 
%\item Use the rule you developed in the problem~\ref{matvect} to compute the following products
%$$
%\begin{pmatrix}
%      1 & 2 & 3 & 4  \\
%      5 & 6  & 7 & 8  \\
%      9 & 10  & 11    & 12  \\
%      13 & 14  & 15 & 16  \\
%    \end{pmatrix}\colvec{1\\2\\ 3 \\4}
%$$
%$$
%\begin{pmatrix}
%      1 & 0& 0 & 0 & 0  \\
%      0 & 1  & 0 & 0 & 0   \\
%      0 & 0  &  1  & 0 & 0  \\
%      0 & 0  &  0&  1 & 0  \\
%      0 & 0 & 0 & 0 & 1
%    \end{pmatrix}\colvec{14\\14\\ 21 \\35\\ 62}
%$$
%$$
%\begin{pmatrix}
%      1 & 42& 97 & 2 & -23 &46  \\
%      0 & 1  & 3 & 1 & 0 & 33 \\
%      11 & \pi  &  1  & 0 & 46 & 29\\
%      -98 & 12  &  0&  33 & 99 & 98\\
%      \log 2 & 0 & \sqrt{2} & 0 & e &23
%    \end{pmatrix}\colvec{0\\0\\ 0 \\0\\ 0\\ 0}
%$$
%$$
%\begin{pmatrix}
%      1 & 2 & 3 & 4  &
%      5 & 6  \\ 7 & 8  &
%      9 & 10  & 11    & 12  \\
%      13 & 14  & 15 & 16  & 17 & 18 
%    \end{pmatrix}\colvec{0\\0\\ 1 \\ 0 \\ 0 \\ 0}
%$$
%Now that you are good at multiplying a matrix with a column vector, try your hand at a product of
%two matrices
%$$
%\begin{pmatrix}
%      1 & 2 & 3 & 4  &
%      5 & 6  \\ 7 & 8  &
%      9 & 10  & 11    & 12  \\
%      13 & 14  & 15 & 16  & 17 & 18 
%    \end{pmatrix}\begin{pmatrix}1& 0 & 0\\0 & 1 & 0\\ 0 & 0 & 1\\ 0& 0 & 0 \\ 0 & 0  & 0\\ 0 & 0 & 0\end{pmatrix}
%$$
%{\it Hint, to do this problem view the matrix on the right 
%as three column vectors next to one another.}
%
%\phantomnewpage

\item  The \emph{standard basis vector} $e_i$ is a column vector with a one in the $i$th row, and zeroes everywhere else.  Using the rule for multiplying a matrix times a vector in problem~\ref{matvect}, find a simple rule for multiplying $Me_i$, where $M$ is the general matrix defined there.

\item If $A$ is a non-linear operator, can the solutions to $Ax=b$ still be written as ``general solution=particular solution + homogeneous solutions"?  Provide examples.

\item Find a system of equations whose solution set is the walls of a $1\times1\times1$  cube. (Hint: You may need to restrict the ranges of the variables; could your equations be linear?)

\end{enumerate}




\phantomnewpage
