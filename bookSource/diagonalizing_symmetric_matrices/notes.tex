
\chapter{\diagSymMatTitle}\label{symmetricmatrices}

Symmetric matrices have many applications.  For example, if we consider the shortest distance between pairs of important cities, we might get a table like the following.
\[
\begin{array}{c|ccc}
 & \text{Davis} & \text{Seattle} 
& \text{San Francisco} \\ \hline
\text{Davis} & 0 & 2000 & 80 \\
\text{Seattle} & 2000 & 0 & 2010 \\
\text{San Francisco} & 80 & 2010 & 0
\end{array}
\]
Encoded as a matrix, we obtain
\[
M=\begin{pmatrix}
\mc{0} & \mc{2000} & \mc{80} \\
\mc{2000} & \mc{0} & \mc{2010} \\
\mc{80} & \mc{2010} & \mc{0}
\end{pmatrix}=M^T.
\]

\begin{definition}
A matrix $M$ is {\bf symmetric}\index{Symmetric matrix} if  $M^T=M.$
\end{definition}

One very nice property of symmetric matrices is that they always have real eigenvalues.  Review exercise~\ref{prob_real_eigenvalues} guides you through the general proof, but below is an example for $2\times 2$ matrices.

\begin{example}
For a general symmetric $2\times 2$ matrix, we have:
\begin{eqnarray*}
P_\lambda \begin{pmatrix} a & b \\ b& d \end{pmatrix}
 &=&
\det\begin{pmatrix}\lambda-a&\mc{-b}\\\mc{-b}&\lambda-d \end{pmatrix}\\[1mm]
&=& (\lambda-a)(\lambda-d)-b^2 \\[2mm]
&=& \lambda^2-(a+d)\lambda-b^2+ad\\[1mm]
\Rightarrow \lambda &=& \frac{a+d}{2}\pm \sqrt{b^2+\left(\frac{a-d}{2}\right)^2}.
\end{eqnarray*}
Notice that the discriminant $4b^2+(a-d)^2$ is always positive, so that the eigenvalues must be real.
\end{example}

Now, suppose a symmetric matrix $M$ has two distinct eigenvalues $\lambda \neq \mu$ and eigenvectors $x$ and $y$;
\[
Mx=\lambda x, \qquad My=\mu y.
\] 
Consider the dot product $x\dotprod y = x^Ty = y^Tx$ and calculate
\begin{eqnarray*}
x^TM y &=& x^T\mu y = \mu x\dotprod y, \text{ and }\\[3mm]
x^TM y &=& (y^TMx)^T \text{ (by transposing a $1\times 1$ matrix)}\\[1mm]
       &=& (y^T\lambda x)^T \\
       &=& (\lambda x\dotprod y)^T \\
             &=& \lambda x\dotprod y.
\end{eqnarray*}
Subtracting these two results tells us that:
\begin{eqnarray*}
0 &=& x^TMy-x^TMy=(\mu-\lambda)\,x\dotprod y.
\end{eqnarray*}
Since $\mu$ and $\lambda$ were assumed to be distinct eigenvalues, $\lambda-\mu$ is non-zero, and so $x\dotprod y=0$.  We have proved the following theorem.

\begin{theorem}
Eigenvectors of a symmetric matrix with distinct eigenvalues are orthogonal.
\end{theorem}

%\begin{center}\href{\webworkurl ReadingHomework23/1/}{Reading homework: problem \ref{symmetricmatrices}.1}\end{center}
\Reading{DiagonalizingSymmetricMatrices}{1}

\begin{example}
The matrix $M=\begin{pmatrix}2&1\\1&2\end{pmatrix}$
has eigenvalues determined by
\[
\det(M-\lambda I)=(2-\lambda)^2-1=0.
\] 
So the eigenvalues of $M$ are $3$ and $1$, and the associated eigenvectors turn out to be 
$\colvec{1\\1}$ and $\colvec{1\\-1}$.  It is easily seen that these eigenvectors are \hyperref[orthogonal]{orthogonal}; 
\[
\colvec{1\\1} \dotprod \colvec{1\\-1}=0.
\]
\end{example}

In \hyperlink{basisorthog}{chapter~\ref{orthonormalbases}} we saw that the matrix $P$ built from any orthonormal basis  $(v_1,\ldots, v_n )$
for ${\mathbb R}^n$ as its columns,
\[
P=\rowvec{v_1 & \cdots & v_n}\, ,
\]
was an orthogonal matrix. This means that 
\[
P^{-1}=P^T, \text{ or } PP^T=I=P^TP.
\]
Moreover, given any (unit) vector $x_1$, one can always find vectors $x_2, \ldots, x_n$ such that $(x_1,\ldots, x_n)$ is an orthonormal basis.  (Such a basis can be obtained using the~\hyperref[GramSchmidt]{Gram-Schmidt procedure}.)

Now suppose $M$ is a symmetric $n\times n$ matrix and $\lambda_1$ is an eigenvalue with eigenvector $x_1$ (this is always the case because every matrix has at least one eigenvalue--see Review Problem~\ref{atleastone}).  
Let $P$ be the square matrix of orthonormal column vectors 
\[
P=\rowvec{x_1 & x_2 & \cdots & x_n},
\]
While $x_1$ is an eigenvector for $M$, the others are not necessarily eigenvectors for $M$.  
Then
\[
MP=\rowvec{\lambda_1 x_1 & Mx_2 & \cdots & Mx_n}.
\]
But $P$ is an orthogonal matrix, so $P^{-1}=P^T$.  Then:
\begin{eqnarray*}
P^{-1}=P^T &=& \ccolvec{x_1^T\\ \vdots \\ x_n^T} \\[1mm]
\Rightarrow P^TMP &=& \begin{pmatrix}
  x_1^T\lambda_1x_1  & * & \cdots & *\\
  x_2^T\lambda_1x_1  & * & \cdots & *\\
  \mc\vdots             &   & & \mc\vdots\\
   x_n^T\lambda_1x_1 & * & \cdots & *\\
  \end{pmatrix}\\[2mm]
&=& \begin{pmatrix}
  \lambda_1  & * & \cdots & *\\
  \mc 0  & * & \cdots & *\\
 \mc \vdots             & *  & & \mc\vdots\\
  \mc 0 & * & \cdots & *\\
  \end{pmatrix}\\[2mm]
&=& \begin{pmatrix}
  \lambda_1  & 0 & \cdots & 0\\
  \mc 0          & & & \\
  \mc\vdots     & & \hat{M} & \\
  \mc0          & & & \\
  \end{pmatrix}\, .\\
\end{eqnarray*}
The last equality follows since $P^TMP$ is symmetric.  The asterisks in the matrix are where ``stuff'' happens; this extra information is denoted by $\hat{M}$ in the final expression.  We know nothing about $\hat{M}$ except that it is an $(n-1)\times (n-1)$ matrix and that it is symmetric.  But then, by finding an (unit) eigenvector for $\hat{M}$, we could repeat this procedure successively.  The end result would be a diagonal matrix with eigenvalues of $M$ on the diagonal. Again, we have proved a theorem: %we also need that every matrix has an eigenvector.

\begin{theorem}
Every symmetric matrix is similar to a diagonal matrix of its eigenvalues.  In other words,
\[
M=M^T \Leftrightarrow M=PDP^T
\]
where $P$ is an orthogonal matrix and $D$ is a diagonal matrix whose entries are the eigenvalues of $M$.
\end{theorem}

%\begin{center}\href{\webworkurl ReadingHomework23/2/}{Reading homework: problem \ref{symmetricmatrices}.2}
\Reading{DiagonalizingSymmetricMatrices}{2}
%\end{center}

To diagonalize a real symmetric matrix, begin by building an orthogonal matrix from an orthonormal basis of eigenvectors, as in the example below. 

\begin{example}
The symmetric matrix 
$$M=\begin{pmatrix}2&1\\1&2\end{pmatrix}\,  ,$$ has eigenvalues $3$ and $1$ with eigenvectors $\colvec{1\\1}$ and $\colvec{1\\-1}$ respectively.  After normalizing these eigenvectors, we  build the orthogonal matrix:
\[
P = \begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\[2mm]
\frac{1}{\sqrt{2}} & \frac{-1}{\sqrt{2}}
\end{pmatrix}\, .
\]
Notice that $P^TP=I$.  Then:
\[
MP = \begin{pmatrix}
\frac{3}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\[2mm]
\frac{3}{\sqrt{2}} & \frac{-1}{\sqrt{2}}
\end{pmatrix} = 
\begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\[2mm]
\frac{1}{\sqrt{2}} & \frac{-1}{\sqrt{2}}
\end{pmatrix} \begin{pmatrix}
3 & 0 \\[2mm]
0 & 1
\end{pmatrix}.
\]
In short, $MP=PD$, so $D=P^TMP$.  Then $D$ is the diagonalized form of $M$ and $P$ the associated change-of-basis matrix from the standard basis to the basis of eigenvectors.
\end{example}

\Videoscriptlink{diagonalizing_symmetric_matrices_3by3_example.mp4}{ $3\times 3$ Example}{scripts_diagonalizing_symmetric_matrices_3by3_example}












%\section*{References}
%Hefferon, Chapter Three, Section V: Change of Basis
%\\
%Beezer, Chapter E, Section PEE, Subsection EHM
%\\
%Beezer, Chapter E, Section SD, Subsection D
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Symmetric_matrix}{Symmetric Matrix}
%\item \href{http://en.wikipedia.org/wiki/Diagonalizable_matrix}{Diagonalizable Matrix}
%\item \href{http://en.wikipedia.org/wiki/Similar_matrix}{Similar Matrix}
%\end{itemize}

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{DiagonalizingSymmetricMatrices}{1}, 
 \hwrref{DiagonalizingSymmetricMatrices}{2}, 
 \\
Diagonalizing a symmetric matrix &  \hwref{DiagonalizingSymmetricMatrices}{3}, \hwref{DiagonalizingSymmetricMatrices}{4}\\
   \hline
\end{tabular}




\input{\diagSymMatPath/problems}

%\newpage
