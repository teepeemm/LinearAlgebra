
\chapter{\linTransTitle}
\label{sec:linearTransformation}

The main objects of study in any course in linear algebra are linear functions:

\begin{definition}
A function $L \colon V\rightarrow W$ is {\bf linear} if $V$ and $W$ are vector spaces and 
\begin{center}
\shabox{$
L(ru + sv) = rL(u) + sL(v) $}
\end{center}
 for all $u,v \in V$ and $r,s \in \Re$.
\end{definition}

\Reading{LinearTransformations}{1}
%\begin{center}\href{\webworkurl ReadingHomework7/1/}{Reading homework: problem 7.1}\end{center}


\begin{remark}
We will often refer to linear functions by names like ``linear map''\index{Linear Map}, ``linear operator''\index{Linear Operator} or ``linear transformation''\index{Linear Transformation}. In some contexts
you will also see the name ``homomorphism''\index{Homomorphism} which generally is applied to functions from one kind of set to the same kind of set while respecting any  structures on the sets; linear maps are from vector spaces to vector spaces that respect scalar multiplication and addition, the two structures on vector spaces. It is common to denote a linear function by capital $L$ as a reminder of its linearity, but sometimes we will use just $f$, after all we are just studying very special functions.
\end{remark}

The definition above coincides with the \hyperlink{twopart}{two part} description in Chapter~\ref{warmup};
the case $r=1,s=1$ describes additivity, while  $s=0$ describes homogeneity. 
We are now ready to learn the powerful consequences of linearity.

\section{The Consequence of Linearity}

Now that we have a sufficiently general notion of vector space 
it is time to talk about why linear operators are so special. 
Think about what is required to fully specify a real function of one variable. 
%We typically deal with functions that have simple algebraic descriptions like 
%$f(x)=x^2$ or $g(x)=\ln(x)$. 
%There are many more functions than these. 
%Imagine assigning a random  output to each of the 
%One output for each input.
%from $\Re^\mathbb{N}$. 
One output must be specified for each input. 
That is an infinite amount of information. 

By contrast, even though a linear function can have infinitely many elements in its domain, it is specified by a very small amount of information. 

\begin{example} (One output specifies infinitely many)\\ 
If you know that the function $L$ is linear and that 
$$L\colvec{1\\0}  =\colvec{5\\3}$$ 
then you do not need any more information to figure out 
$$L\colvec{2\\0},~L\colvec{3\\0}~,L\colvec{4\\0},~L\colvec{5\\0} ,{\rm ~\mbox{\it etc}}\ldots, $$ 
because by homogeneity
$$L\colvec{5\\0}=L\left[ 5\colvec{1\\0} \right] = 5 L\colvec{1\\0}=5\colvec{5\\3}=\colvec{25\\15}.$$
In this way an an infinite number of outputs is specified by just one.
\end{example}

\begin{example}(Two outputs in $\mathbb{R}^2$ specifies all outputs)\\
Likewise, if you  know that $L$ is linear and that
$$
L\colvec{1\\0}=\colvec{5\\3} {\rm ~and~} L\colvec{0\\1}= \colvec{2\\2}
$$ 
then you don't need any more information to compute
$$L\colvec{1\\1}$$ because by additivity
$$
L\colvec{1\\1}= L \left[ \colvec{1\\0} + \colvec{0\\1} \right] 
=L\colvec{1\\0} + L \colvec{0\\1} = \colvec{5\\3} +\colvec{2\\2} =\colvec{7\\5}.
$$
In fact, since every vector in $\Re^2$ can be expressed as 
$$
\colvec{x\\y}= x\colvec{1\\0}+y\colvec{0\\1}\, ,
$$ 
we know how $L$ acts on every vector from 
$\Re^2$ by linearity based on just  two pieces of information;
$$
L\colvec{x\\y}
= L \left[ x\colvec{1\\0}+y\colvec{0\\1} \right]
= x L\colvec{1\\0}+y L\colvec{0\\1} 
= x \colvec{5\\3}+ y\colvec{2\\2} =\colvec{5x+2y\\3x+2y}.
$$
Thus, the value of $L$ at infinitely many inputs is completely specified by its value at just two inputs.
(We can see now that $L$ acts in exactly the way the matrix 
$$
\begin{pmatrix}
5&2\\
3&2 \end{pmatrix}
$$
acts on vectors from $\Re^2$.)
\end{example}

\Reading{LinearTransformations}{2}

This is the reason that linear functions are so nice;
they are secretly very simple functions by virtue of two characteristics:
\begin{enumerate}\item They act on vector spaces.
\item They act additively and homogeneously. 
\end{enumerate}


A linear transformation with domain $\Re^3$ is completely specified by the way it acts on the three vectors 
$$
\colvec{1\\0\\0}\, ,\:\colvec{0\\1\\0}\, ,\:\colvec{0\\0\\1}\, .
$$
Similarly, a linear transformation with domain $\Re^n$ is completely specified by its action on the $n$ different $n$-vectors that have exactly one non-zero component, and its matrix form can be read off this information. However, not all linear functions have such nice domains.


%%%%%%%%%%%%%%%%%%%%%%

\section{Linear Functions on Hyperplanes }
It is not always so easy to write a linear operator as a matrix. 
Generally, this will amount to solving a linear systems problem. Examining a linear function whose domain is a hyperplane is instructive.

\begin{example}\label{Vdef} Let $$V=\left\{  c_1\colvec{1\\1\\0} +c_2\colvec{0\\1\\1} \middle| c_1,c_2\in \Re \right\} $$ and consider $L:V\to \Re^3$ be a linear function that obeys 
$$
L\colvec{1\\1\\0 } = \colvec{0\\1\\0},\qquad
L\colvec{0\\1\\1 } = \colvec{0\\1\\0}.
$$
By linearity this specifies the action of $L$ on any vector from $V$ as
$$
L\left[ c_1\colvec{1\\1\\0 } + c_2 \colvec{0\\1\\1} \right]= (c_1+c_2)\colvec{0\\1\\0}.
$$
The domain of $L$ is a plane and its range is the line through the origin in the $x_2$ direction. 


It is not clear how to formulate $L$ as a matrix; 
since
\begin{eqnarray*}
L\ccolvec{c_1\\\!\!c_1+c_2\!\!\\c_2} = 
\begin{pmatrix}
0&0&0\\
1&0&1\\
0&0&0
\end{pmatrix}
\ccolvec{c_1\\\!\!c_1+c_2\!\!\\c_2} =(c_1+c_2)\colvec{0\\1\\0}\, ,
\end{eqnarray*}
{\it or} 
\begin{eqnarray*}
L\ccolvec{c_1\\\!\!c_1+c_2\!\!\\c_2} = 
\begin{pmatrix}
0&0&0\\
0&1&0\\
0&0&0
\end{pmatrix}
\ccolvec{c_1\\\!\!c_1+c_2\!\!\\c_2} =(c_1+c_2)\colvec{0\\1\\0}\, ,
\end{eqnarray*}
you might suspect that  $L$ is equivalent to one of these $3\times3$ matrices. It is not. By the natural domain convention, all $3\times3$ matrices have $\Re^3$ as their domain, and the domain of $L$ is smaller than that. 
When we do realize this $L$ as a matrix it will be as a  $3\times2$ matrix. We can tell because the domain of $L$ is 2 dimensional and the codomain is $3$ dimensional. (You probably already know that the plane has dimension~2, and a line is 1~dimensional, but the careful definition of ``dimension'' takes some work; this is tackled in Chapter~\ref{basisdimension}.) This leads us to write
$$
L\left[ c_1\colvec{1\\1\\0 } + c_2 \colvec{0\\1\\1} \right]=c_1\colvec{0\\1\\0}+c_2\colvec{0\\1\\0}=\begin{pmatrix}0&0\\1&1\\0&0\end{pmatrix}\colvec{c_1\\c_2}\, .
$$
This makes sense, but requires a {\it warning}: The matrix $\begin{pmatrix}0&0\\1&1\\0&0\end{pmatrix}$ specifies $L$  so long as you also provide the information that you are labeling points in the plane $V$
by the two numbers $(c_1,c_2)$.
\end{example} 




%Recall that the key properties of vector spaces are vector addition and scalar multiplication.  Now suppose we have two vector spaces $V$ and $W$ and a map $L$ between them:
%\[
%L \colon V \rightarrow W
%\]
%Now, both $V$ and $W$ have notions of vector addition and scalar multiplication.  It would be ideal if the map $L$ \emph{preserved} these operations.  In other words, if adding vectors and then applying $L$ were the same as applying $L$ to vectors and then adding them.  Likewise, it would be nice if, when multiplying by a scalar, it didn't matter whether we multiplied before or after applying~$L$.  In formulas, this means that for any $u,v \in V$ and $c \in \Re$:
%\begin{eqnarray*}
%L(u+v) &=& L(u)+L(v) \\[2mm]
%L(cv) &= &cL(v)
%\end{eqnarray*}
%
%Combining these two requirements into one equation, we get the definition of a linear function\index{Linear function} or linear transformation\index{Linear transformation}.


%Notice that on the left the addition and scalar multiplication occur in $V$, while on the right the operations occur in $W$.
%This is often called the {\it linearity property}\index{Linearity} of a linear transformation.



%\begin{example}
%Take $L \colon \Re^3\rightarrow \Re^3$ defined by:
%
%\[
%L\colvec{x\\y\\z} = \colvec{ x+y\\y+z\\0 }
%\]
%The domain of $L$ is $\Re^3$. The range is not all of $\Re^3$, but just the plane comprised of vecotrs whose third component is zero. We say that $L$ transforms $\Re^3$ into this plane.\\
%
%We now check linearity.  Call $u = \colvec{x\\y\\z}$ and $v=\colvec{a\\b\\c}$.  
%
%\begin{eqnarray*}
%L(ru + sv) & = & L\left( r \colvec{x\\y\\z} + s \colvec{a\\b\\c} \right) \\
% & = & L\left( \colvec{rx\\ry\\rz} + \colvec{sa\\sb\\sc} \right) \\
% & = & L \colvec{rx+sa\\ry+sb\\rz+sx}  \\
% & = & \colvec{rx+sa+ry+sb\\ry+sb+rz+sx\\0}
%\end{eqnarray*}
%On the other hand,
%
%\begin{eqnarray*}
%rL(u) + sL(v) & = & rL\colvec{x\\y\\z} + sL\colvec{a\\b\\c}\\
% & = & r\colvec{x+y\\y+z\\0} + s\colvec{a+b\\b+c\\0}\\
% & = & \colvec{rx+ry\\ry+rz\\0} + \colvec{sa+sb\\sb+sc\\0}\\
% & = & \colvec{rx+sa+ry+sb\\ry+sb+rz+sx\\0}
%\end{eqnarray*}
%Then the two sides of the linearity requirement are equal, so $L$ is a linear transformation.
%
%We can write the linear transformation $L$ in the previous example using a matrix like so:
%\[
%L\colvec{x\\y\\z} = \colvec{x+y\\y+z\\0} = 
%\begin{pmatrix}
%1 & 1 & 0 \\
%0 & 1 & 1 \\
%0 & 0 & 0 \\
%\end{pmatrix}\colvec{x\\y\\z}
%\]
%
%\begin{center}\href{\webworkurl ReadingHomework7/2/}{Reading homework: problem 7.2}\end{center}
%\end{example}

%\videoscriptlink{linear_transformations_example.mp4}{A linear and non-linear example}{scripts_linear_transformations_example}

\section{Linear Differential Operators}

Your calculus class became much easier when you stopped using the limit definition of the derivative,  learned the power rule, and started using linearity of the derivative operator.

\begin{example}
Let $V$ be the vector space of polynomials of degree 2 or less with standard addition and scalar multiplication;
\[
V := \{a_0\cdot1 + a_1x + a_2 x^2 \, | \,  a_0,a_1,a_2 \in \Re \}
\]
Let $\frac{d}{dx} \colon V\rightarrow V$ be \hypertarget{derivative_linear}{the derivative operator.}  
The following three equations, along with linearity of the derivative operator, allow one to take the derivative of any 2nd degree polynomial:
$$
\frac{d}{dx} 1=0,~\frac{d}{dx}x=1,~\frac{d}{dx}x^2=2x\,. 
$$
In particular
$$
\frac{d}{dx} (a_01 + a_1x + a_2 x^2) = 
 a_0\frac{d}{dx}1 + a_1 \frac{d}{dx} x + a_2 \frac{d}{dx} x^2  
 = 0+a_1+2a_2x.
$$
Thus, the derivative acting any of the infinitely many second order polynomials is determined by its action for just three inputs.
%The full statement of linearity of $\frac{d}{dx}$ is:  
%For 2nd order polynomials $p_1,p_2$ and numbers $r,s$, 
%\[
%\frac{d}{dx}(rp_1 + s p_2) = r \frac{dp_1}{dx} + s \frac{dp_2}{dx} .
%\]
%We can represent a polynomial as a ``semi-infinite vector'', like so:
%\[
%a_0 + a_1x + \cdots + a_nx^n \longleftrightarrow 
%\colvec{ a_0 \\ a_1 \\ \vdots \\ a_n \\ 0 \\ 0 \\ \vdots }
%\]

%Then we have:
%\[
%\frac{d}{dx}(a_0 + a_1x + \cdots + a_nx^n) = a_1 + 2a_2x + \cdots + na_{n}x^{n-1} \\
%\longleftrightarrow 
%\colvec{ a_1 \\ 2a_2 \\ \vdots \\ na_n \\ 0 \\ 0 \\ \vdots }
%\]
%
%One could then write the derivative as an ``infinite matrix'':
%\[
%\frac{d}{dx} \longleftrightarrow 
%\begin{pmatrix}
%0 & 1 & 0 & 0 & \cdots \\
%0 & 0 & 2 & 0 & \cdots \\
%0 & 0 & 0 & 3 & \cdots \\
%\vdots & \vdots & \vdots & \vdots &  \\
%\end{pmatrix}
%\]
\end{example}


\section{Bases (Take 1)} 
The central idea of linear algebra is to exploit the hidden simplicity of linear functions. 
It ends up there is a lot of freedom in how to do this. That freedom is what makes linear algebra powerful.

You saw that a linear operator acting on $\Re^2$ is completely specified by how it acts on the pair of vectors $\colvec{1\\0}$ and $\colvec{0\\1}$. 
In fact, any linear operator acting on $\Re^2$ is also completely specified by how it acts on the pair of vectors $\colvec{1\\1}$ and $\colvec{1\\-1}$.

\begin{example} The linear operator $L$ is a linear operator then it is completely specified \hypertarget{nonstandard r2 basis}{by the two equalities} 
$$
L\colvec{1\\1}= \colvec{2\\4}, {\rm ~and~} L\colvec{1\\-1}=\colvec{6\\8}.
$$ 
This is because any vector $\colvec{x\\y}$ in $\Re^2$ is a sum of multiples of
$\colvec{1\\1}$ and $\colvec{1\\-1}$ which can be calculated via a linear systems problem as follows:
\begin{eqnarray*}&&
\colvec{x\\y}=a\colvec{1\\1}+b\colvec{1\\-1}\\[1mm]
&\Leftrightarrow& 
\begin{pmatrix}
1&1\\
1&-1
\end{pmatrix}
\colvec{a\\b}
=\colvec{x\\y}\\[1mm]
&\Leftrightarrow& 
\begin{amatrix}{2}
1&1&x\\
1&-1&y
\end{amatrix}
\sim \begin{amatrix}{2}
1&0& \frac{x+y}{2}\\
0&1&\frac{x-y}2
\end{amatrix}\\[1mm]
&\Leftrightarrow&
\left\{ 
\begin{array}{l}
a=\frac{x+y}{2}\\
b=\frac{x-y}{2}\, .
\end{array}
\right.
\end{eqnarray*}
Thus
$$
\colvec{x\\[2mm]y}=\frac{x+y}{2}\colvec{1\\[2mm]1}+\frac{x-y}{2}\colvec{1\\[2mm]-1}\, .
$$
We can then calculate how $L$ acts on any vector by first expressing the vector as  a sum of multiples and then applying linearity;
\begin{eqnarray*}
L\colvec{x\\y}
&=&L\left[    \frac{x+y}{2} \colvec{1\\1} + \frac{x-y}{2} \colvec{1\\-1}  \right]\\[1mm]
&=&\frac{x+y}{2} L \colvec{1\\1} + \frac{x-y}{2} L \colvec{1\\-1} \\[2mm]
&=&\frac{x+y}{2} \colvec{2\\4} + \frac{x-y}{2}  \colvec{6\\8} \\[1mm]
&=&\ccolvec{x+y \\ 2(x+y)} +  \colvec{3(x-y)\\4(x-y)}\\[1mm]
&=&\ccolvec{4x-2y \\ 6x-2y}
\end{eqnarray*}
Thus $L$ is completely specified by its value at just two inputs. 
%In fact, we find that $L$ is equivalent to a matrix, 
%$$
%L\colvec{a\\b}=
%\begin{pmatrix}
%4&-2\\
%6&-1
%\end{pmatrix}
%\colvec{a\\b}.
%$$
%but only by virtue of it having domain and targert $\Re^2$.
\end{example}

It should not surprise you to learn there are infinitely many pairs of vectors from $\Re^2$ 
with the property that any vector can be expressed as a linear combination of them; any pair that when used as columns of a matrix gives an invertible matrix works. Such a pair is called a {\it basis}\index{basis} for $\Re^2$.

Similarly, there are infinitely many triples of vectors with the property that any vector from $\Re^3$ can be expressed as a linear combination of them: these are the triples that used as columns of a matrix give an invertible matrix. Such a triple is called a basis for $\Re^3$.

In a similar spirit, there are infinitely many pairs of vectors with the property that every vector in 
$$V=\left\{  c_1\colvec{1\\1\\0} +c_2\colvec{0\\1\\1} \middle\vert \, c_1,c_2\in \Re \right\} $$ 
can be expressed as a linear combination of them. Some examples are 
$$V=
\left\{ c_1\colvec{1\\1\\0} +c_2\colvec{0\\2\\2}  \middle\vert c_1,c_2\in \Re \right\} 
=\left\{c_1 \colvec{1\\1\\0}+c_2 \colvec{1\\3\\2}  \middle\vert c_1,c_2\in \Re \right\} 
%\\
%=\left\{  c_1\colvec{2\\4\\2} +c_2 \colvec{1\\3\\2}  | c_1,c_2\in \Re \right\} 
$$
Such a pair is a  called a basis for $V$.



%\begin{remark}[Foreshadowing Dimension.]

You probably have some intuitive notion of what dimension\index{Dimension!concept of} means
(the careful mathematical definition is given in chapter~\ref{sec:dimension}).
%Some of the examples of vector spaces we have worked with have been finite dimensional.  (For example, $\Re^n$ will turn out to have dimension $n$.)  
%The polynomial example above is an example of an infinite dimensional vector space.  
Roughly speaking, dimension is the number of independent directions available.  To figure out the dimension of a vector space, I stand at the origin, and pick a direction.  If there are any vectors in my vector space that aren't in that direction, then I choose another direction that isn't in the line determined by the direction I chose.  If there are any vectors in my vector space not in the plane determined by the first two directions, then I choose one of them as my next direction.  In other words, I choose a collection of \emph{independent} vectors in the vector space (independent vectors are defined in Chapter~\ref{linearind}).  
A minimal set of independent vectors is called a {\it basis}\index{basis} (see Chapter~\ref{basisdimension} for the precise definition). 
The number of vectors in my basis is the dimension of the vector space. 
Every vector space has many bases, but all bases for a particular vector space have the same number of vectors. Thus dimension is a well-defined concept. 

The fact that every vector space (over~$\Re$) has infinitely many bases is actually very useful. 
Often a good choice of  basis can reduce the time required to run a calculation in dramatic ways! 

In summary:

\begin{center}
\shabox{A basis is a set of vectors in terms of which it is possible to uniquely express any other vector.}
\end{center}
%For finite dimensional vector spaces, linear transformations can always be represented by matrices.  For that reason, we will start studying matrices intensively in the next few lectures.
%\end{remark}


%\section*{References}
%
%Hefferon, Chapter Three, Section II.  (Note that Hefferon uses the term \emph{homomorphism} for a linear map.  `Homomorphism' is a very general term which in mathematics means `Structure-preserving map.'  A linear map preserves the linear structure of a vector space, and is thus a type of homomorphism.)
%\\[2mm]
%Beezer, Chapter LT, Section LT, sections LT, LTC, and MLT.
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Linear_transformation}{Linear Transformation}
%\item \href{http://en.wikipedia.org/wiki/Dimension_(linear_algebra)}{Dimension}
%\end{itemize}
%

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading problems &
\hwrref{LinearTransformations}{1}, \hwrref{LinearTransformations}{2}\\
Linear? & \hwref{LinearTransformations}{3}\\
Matrix $\times$ vector & \hwref{LinearTransformations}{4}, \hwref{LinearTransformations}{5}\\
Linearity & \hwref{LinearTransformations}{6}, \hwref{LinearTransformations}{7}\\
\hline
\end{tabular}
\input{\linTransPath/problems}

\newpage
